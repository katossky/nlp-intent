{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset silicone (/Users/katossky/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe4ae80f5fc4f75866dbeb88aff4fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Based on roBERTa and using the transformers library\n",
    "# and the https://huggingface.co/datasets/silicone dataset\n",
    "# train a classifier of each \"utterance\" into the categories 0 to 3\n",
    "\n",
    "# Ideally, the classification should depend on the dialog context\n",
    "# using the Dialogue_ID and Idx variables\n",
    "# respectively giving the dialogue identifier and the utterance order.\n",
    "\n",
    "# Inspired by https://huggingface.co/docs/transformers/training\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.load_dataset('silicone', 'dyda_da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87170"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Utterance': 'say , jim , how about going for a few beers after dinner ?',\n",
       " 'Dialogue_Act': 'directive',\n",
       " 'Dialogue_ID': '1',\n",
       " 'Label': 1,\n",
       " 'Idx': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(doc):\n",
    "    return tokenizer(doc[\"Utterance\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb60c36284e4f64936ae9d059a5f81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/katossky/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5/cache-02fc435480a05403.arrow\n",
      "Loading cached processed dataset at /Users/katossky/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5/cache-c2d579ded85024ea.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = dataset.rename_column(\"Label\", \"label\").map(tokenize_function, batched=True).remove_columns(['Utterance', 'Dialogue_Act', 'Dialogue_ID', 'Idx'])\n",
    "small_train_dataset = dataset_tokenized['train'] # .shuffle(seed=42).select(range(500))\n",
    "small_test_dataset  =  dataset_tokenized['test'] # .shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87170, 7740)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(small_train_dataset), len(small_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Utterance', 'Dialogue_Act', 'Dialogue_ID', 'label', 'Idx', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "training_args = TrainingArguments(\n",
    "    seed=42,                         # random seed for initialization\n",
    "    output_dir='./results',          # output directory\n",
    "    do_train=True,                   # do training\n",
    "    evaluation_strategy=\"steps\",     # evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # evaluation step\n",
    "    #num_train_epochs=3,              # total # of training epochs\n",
    "    #per_device_train_batch_size=16,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    #warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    #weight_decay=0.01,               # strength of weight decay\n",
    "    log_level='debug',\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,               # eval_steps is set to the same value\n",
    "    save_strategy=\"epoch\",\n",
    "    auto_find_batch_size=True,       # automatically find the best batch size\n",
    "    use_mps_device=True,             # use the new Apple M1 chip\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=small_train_dataset,         # training dataset\n",
    "    eval_dataset=small_test_dataset,           # evaluation dataset\n",
    "    # tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Idx, Dialogue_Act, Utterance, Dialogue_ID. If Idx, Dialogue_Act, Utterance, Dialogue_ID are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/katossky/Projets/ensae/a3-s2-nlp/project-3-intent/.venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 87170\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32691\n",
      "  Number of trainable parameters = 124648708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7a349c14dd4fc6bc5fed8b07b3e098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Idx, Dialogue_Act, Utterance, Dialogue_ID. If Idx, Dialogue_Act, Utterance, Dialogue_ID are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7740\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7563, 'learning_rate': 4.9847052705637644e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ad55f6ec39447ca6d177c190c96c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6663451790809631, 'eval_accuracy': 0.7465116279069768, 'eval_runtime': 304.8435, 'eval_samples_per_second': 25.39, 'eval_steps_per_second': 3.175, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Idx, Dialogue_Act, Utterance, Dialogue_ID. If Idx, Dialogue_Act, Utterance, Dialogue_ID are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7740\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7472, 'learning_rate': 4.969410541127528e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8506ab2cd9bc408cb9d4b2dca553288d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.711063802242279, 'eval_accuracy': 0.7024547803617571, 'eval_runtime': 304.3257, 'eval_samples_per_second': 25.433, 'eval_steps_per_second': 3.181, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Idx, Dialogue_Act, Utterance, Dialogue_ID. If Idx, Dialogue_Act, Utterance, Dialogue_ID are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7740\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.675, 'learning_rate': 4.954115811691292e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cecb33ae13148c4b98bd9e2d49fe6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6733798384666443, 'eval_accuracy': 0.7804909560723514, 'eval_runtime': 306.6239, 'eval_samples_per_second': 25.243, 'eval_steps_per_second': 3.157, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: Idx, Dialogue_Act, Utterance, Dialogue_ID. If Idx, Dialogue_Act, Utterance, Dialogue_ID are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7740\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7474, 'learning_rate': 4.9388210822550554e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3e441b76cb4411af46de71e06dc159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (4.26.1)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.9/site-packages (8.0.4)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.9/site-packages (0.16.0)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.9/site-packages (0.38.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp39-cp39-macosx_12_0_arm64.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (8.11.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (6.21.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.venv/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./.venv/lib/python3.9/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in ./.venv/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp39-cp39-macosx_12_0_arm64.whl (28.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.9/28.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: appnope in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: pyzmq>=20 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.0.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pickleshare in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: backcall in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in ./.venv/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.0.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.15.0)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.1 scipy-1.10.1 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch transformers ipywidgets datasets accelerate evaluate wheel scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4aea6b1b09ae404ba30e980ce24df721b0c73dc6eae7810ca3151147d99a9ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
