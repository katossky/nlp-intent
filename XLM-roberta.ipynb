{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfadbc82-4861-49b9-a6d7-47c1bbee4348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775f6d57-00bc-474c-90f9-a5573864d84b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset open_subtitles (/home/onyxia/.cache/huggingface/datasets/open_subtitles/en-fr-lang1=en,lang2=fr/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"open_subtitles\", split=\"train[:10%]\", lang1=\"en\", lang2=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934797e-df9d-4a5b-89e0-c0ccf3487249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259d6186-ad0c-40e1-8795-70183f20a7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = train_data.select(range(500, 600))\n",
    "train_data = train_data.select(range(100000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f792ba-adba-4722-b59e-a25cf9484617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'meta', 'translation'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf703bf5-60b1-46b7-bef4-b3f73b6cdaf0",
   "metadata": {},
   "source": [
    "## VÃ©rifier paramÃ¨tre entrainable et modifier la boucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe52915d-218b-4962-8a2d-3470eca18b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = XLMRobertaForMaskedLM.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b1b63c-8003-4adc-8284-87ca4d59c39a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters before freezing layers: 278295186\n"
     ]
    }
   ],
   "source": [
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters before freezing layers:\", num_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f1857b-6e19-44e8-bfb7-8eceaa3204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Freeze all layers except the last 3 layers\n",
    "for name, param in model.named_parameters():\n",
    "    if 'encoder' in name:\n",
    "        # Check if the last character of the name is a digit\n",
    "        if name[-1].isdigit():\n",
    "            layer_num = int(name[-1])\n",
    "            # Fine-tune the last 3 layers of the encoder\n",
    "            if layer_num >= 9:\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            # Skip this parameter if it doesn't have a valid layer number\n",
    "            continue\n",
    "    else:\n",
    "        # Freeze all other layers\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8ba124-8595-470c-8c81-650830ead695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 85054464\n"
     ]
    }
   ],
   "source": [
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters:\", num_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeaa1b7-6f8a-41bc-abb4-9d9da0136005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e43bd8-7d5b-4447-ba02-ff7914521ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of XLMRobertaForMaskedLM(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): XLMRobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9d285-ee22-4fb1-ba45-16ec4f0dbcff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the OpenSubtitles dataset\n",
    "\n",
    "# Preprocess the sentences to create a cloze dataset\n",
    "def preprocess_sentence(sentence):\n",
    "    tokenized = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    for i, token_id in enumerate(tokenized):\n",
    "        masked_tokenized = list(tokenized)\n",
    "        masked_tokenized[i] = tokenizer.mask_token_id\n",
    "        yield masked_tokenized, [token_id] + [-100] * (len(tokenized) - 1)\n",
    "\n",
    "train_cloze_data = []\n",
    "\n",
    "text = train_data['translation']\n",
    "\n",
    "list_of_spoken_english_sentences = []\n",
    "\n",
    "# Recuperer les uniquement les phrases. \n",
    "for elem in text:\n",
    "    \n",
    "    sentence = elem[\"en\"]\n",
    "    train_cloze_data += list(preprocess_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06e4dbc-caaf-40c2-b244-0343cde73f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = train_data['translation']\n",
    "\n",
    "list_of_spoken_english_sentences = []\n",
    "\n",
    "# Recuperer les uniquement les phrases. \n",
    "for d in text:\n",
    "    \n",
    "    tex = d[\"en\"]\n",
    "    list_of_spoken_english_sentences.append(tex)\n",
    "\n",
    "# Print sentencessss\n",
    "#print(list_of_spoken_english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4b6eaa-9160-4001-bc5b-31c6c3fc5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def get_cloze_dataset(sentences: List[str]) -> List[Tuple[str, str, str]]:\n",
    "    cloze_dataset = []\n",
    "    for sentence in sentences:\n",
    "        # Split the sentence into tokens\n",
    "        tokens = sentence.split() # utiliser un autre tokenizer ! \n",
    "\n",
    "        # Loop over the tokens and create a cloze sentence for each\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Create a copy of the tokens with the current token masked out\n",
    "            masked_tokens = tokens[:]\n",
    "            masked_tokens[i] = \"[MASK]\"\n",
    "\n",
    "            # Create the input sentence, masked sentence, and label\n",
    "            input_sentence = \" \".join(tokens)\n",
    "            masked_sentence = \" \".join(masked_tokens)\n",
    "            label = token\n",
    "\n",
    "            # Add the tuple to the cloze dataset\n",
    "            cloze_dataset.append(( masked_sentence,input_sentence, label))\n",
    "    return cloze_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0351913b-941a-4e53-833b-9c4c0a6ed52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_cloze_dataset(list_of_spoken_english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf11e77-326c-4ce0-bfe2-8e0602d0e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f0fd75c-f238-4aad-b81b-6ff866012e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_index(inputs):\n",
    "    masked_index = torch.where(inputs['input_ids'][0] == tokenizer.mask_token_id)[0][0]\n",
    "    return masked_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6465dc4-34af-48d0-a67e-ebf4fad862a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_word_tensor(masked_word):\n",
    "    masked_word_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(masked_word))\n",
    "    return masked_word_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b4a6862-be36-42a7-9e61-2e234cf369c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_masked_token(inputs, masked_word_tensor):\n",
    "    masked_index = get_masked_index(inputs)\n",
    "    inputs['input_ids'][0][masked_index] = masked_word_tensor\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a06ee59d-419e-4cd1-bcba-c785c729225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(inputs):\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    masked_index = get_masked_index(inputs)\n",
    "    labels[0][masked_index] = -100\n",
    "    return labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56dc1137-06a1-4485-834a-42ab24300f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClozeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        masked_sentence, sentence, masked_word = self.data[index]\n",
    "        \n",
    "        inputs = tokenize_sentence(masked_sentence)\n",
    "        masked_word_tensor = get_masked_word_tensor(masked_word)\n",
    "        inputs = replace_masked_token(inputs, masked_word_tensor)\n",
    "        labels = create_labels(inputs)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'][0],\n",
    "            'attention_mask': inputs['attention_mask'][0],\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa807f0-2b30-40df-b928-5e9b1ec7ec62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865e085-3094-4c05-9620-2b4e035079f6",
   "metadata": {},
   "source": [
    "### https://towardsdatascience.com/transformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1270400c-e228-4d04-ab67-77c26102d5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = test_data['translation']\n",
    "\n",
    "list_of_spoken_english_sentences_test = []\n",
    "\n",
    "# Recuperer les uniquement les phrases. \n",
    "for d in text:\n",
    "    \n",
    "    tex = d[\"en\"]\n",
    "    list_of_spoken_english_sentences_test.append(tex)\n",
    "\n",
    "# Print sentencessss\n",
    "#print(list_of_spoken_english_sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ffb9a8-042b-440e-b480-0d0e94e493b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "with open('my_file_train.txt', 'w') as file:\n",
    "    for item in list_of_spoken_english_sentences:\n",
    "        file.write(f\"{item}\\n\")\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ba14f1-7875-4c63-8234-e01cb4282500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with open('my_file_test.txt', 'w') as file:\n",
    "    for item in list_of_spoken_english_sentences_test:\n",
    "        file.write(f\"{item}\\n\")\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa4975f-edf3-4725-839c-3c73f0c89fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"my_file_train.txt\",\n",
    "    block_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0916a8a-0213-4ccb-89b2-71ec66ef7fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"my_file_test.txt\",\n",
    "    block_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf9895b-1718-4e23-87b2-01cc64a7c83c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,    87,  8306, 48869,   297,  8108,     2])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b221dd-fb30-4597-8991-f1222a92479b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b020d94-043f-4dd2-ba50-d4790092f8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert arrays to tensors\n",
    "    predictions = torch.from_numpy(predictions)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    # Calculate the test loss\n",
    "    loss = nn.CrossEntropyLoss()(predictions, labels).item()\n",
    "    return {\"test_loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89daff85-33ca-4ee8-9a87-1ecec185d729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta-trained\",\n",
    "    overwrite_output_dir=True,\n",
    "    #evaluation_strategy = \"steps\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    seed=1,\n",
    "    #eval_steps= 50,\n",
    "    #evaluation_strategy = \"steps\",\n",
    "    #eval_accumulation_steps = 16,\n",
    "    learning_rate=0.000001\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    #eval_dataset = dataset_test,\n",
    "    #compute_metrics=compute_metrics\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bfabb82-f63e-421f-b653-4e5cb0df02f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 37500\n",
      "  Number of trainable parameters = 85054464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37500/37500 1:23:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>1.920600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.011800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./roberta-retrained/checkpoint-1000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-1000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-2000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-2000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-3000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-3000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-4000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-4000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-5000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-5000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-6000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-6000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-7000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-7000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-8000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-8000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-9000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-9000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-10000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-10000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-11000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-11000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-12000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-12000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-13000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-13000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-14000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-14000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-15000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-15000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-16000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-16000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-17000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-17000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-18000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-18000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-19000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-19000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-20000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-20000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-21000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-21000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-22000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-22000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-23000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-23000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-24000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-24000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-25000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-25000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-26000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-26000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-27000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-27000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-28000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-28000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-29000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-29000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-30000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-30000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-31000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-31000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-32000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-32000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-33000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-33000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-34000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-34000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-35000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-35000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-36000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-36000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./roberta-retrained/checkpoint-37000\n",
      "Configuration saved in ./roberta-retrained/checkpoint-37000/config.json\n",
      "Model weights saved in ./roberta-retrained/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-retrained/checkpoint-34000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=37500, training_loss=2.093136355794271, metrics={'train_runtime': 4984.4995, 'train_samples_per_second': 60.187, 'train_steps_per_second': 7.523, 'total_flos': 3747969054813600.0, 'train_loss': 2.093136355794271, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5e073f6-1035-4126-9582-71f0c88eb1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, RobertaForMaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b830445-250f-4e24-8e5f-72a0a29aaaae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./roberta-retrained/checkpoint-12000/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"./roberta-retrained/checkpoint-12000\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"./roberta-retrained/checkpoint-12000\")\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a4387a7-9154-4f8c-9f42-b2e88cce6974",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/onyxia/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/onyxia/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of XLMRobertaForMaskedLM were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = XLMRobertaForMaskedLM.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "073727cd-da5f-4711-af03-02c088b9a643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2370229959487915,\n",
       "  'token': 22317,\n",
       "  'token_str': 'links',\n",
       "  'sequence': 'Send these links back!'},\n",
       " {'score': 0.06669365614652634,\n",
       "  'token': 55769,\n",
       "  'token_str': 'items',\n",
       "  'sequence': 'Send these items back!'},\n",
       " {'score': 0.06505978852510452,\n",
       "  'token': 6305,\n",
       "  'token_str': 'tips',\n",
       "  'sequence': 'Send these tips back!'},\n",
       " {'score': 0.06319175660610199,\n",
       "  'token': 69141,\n",
       "  'token_str': 'tags',\n",
       "  'sequence': 'Send these tags back!'},\n",
       " {'score': 0.04327935352921486,\n",
       "  'token': 4136,\n",
       "  'token_str': 'cookies',\n",
       "  'sequence': 'Send these cookies back!'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model= model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "fill_mask(\"Send these <mask> back!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41190b27-2e61-4d31-adb9-59b9914390e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./XLM-roberta-retrained-EN/checkpoint-37000/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file ./XLM-roberta-retrained-EN/checkpoint-37000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForMaskedLM.\n",
      "\n",
      "All the weights of XLMRobertaForMaskedLM were initialized from the model checkpoint at ./XLM-roberta-retrained-EN/checkpoint-37000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForMaskedLM\n",
    "\n",
    "model_path = \"./XLM-roberta-retrained-EN/checkpoint-37000\"\n",
    "model = XLMRobertaForMaskedLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30bde96f-341e-4229-93de-48ddb068932a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings', 'encoder'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta._modules.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "816824bf-e244-4c9d-942a-ad5623796f70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "lm_head.bias\n",
      "lm_head.dense.weight\n",
      "lm_head.dense.bias\n",
      "lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias\n",
      "Number of trainable parameters: 278295186\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters:\", num_trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "241ba228-e1bd-4586-ba11-ab5e62058fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roberta.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d73cf362-e2ee-4341-8565-1795de5dab48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./XLM-roberta-retrained-EN/checkpoint-37000/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"./XLM-roberta-retrained-EN/checkpoint-37000\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file ./XLM-roberta-retrained-EN/checkpoint-37000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./XLM-roberta-retrained-EN/checkpoint-37000 were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ./XLM-roberta-retrained-EN/checkpoint-37000 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./XLM-roberta-retrained-EN/checkpoint-37000\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06871b-86ce-42b1-95cc-43748601e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params = [sum(p.numel() for p in layer.parameters() if p.requires_grad) for layer in model.encoder.layer[-3:]]\n",
    "print(\"Number of trainable parameters in last 3 layers:\", num_trainable_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
